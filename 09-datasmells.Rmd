# Data Cleaning Part I: Data smells

Any time you are given a dataset from anyone, you should immediately be suspicious. Is this data what I think it is? Does it include what I expect? Is there anything I need to know about it? Will it produce the information I expect?

One of the first things you should do is give it the smell test. 

Failure to give data the smell test [can lead you to miss stories and get your butt kicked on a competitive story](https://source.opennews.org/en-US/learning/handling-data-about-race-and-ethnicity/).

With data smells, we're trying to find common mistakes in data. [For more on data smells, read the GitHub wiki post that started it all](https://github.com/nikeiubel/data-smells/wiki/Ensuring-Accuracy-in-Data-Journalism). Some common data smells are:

* Missing data or missing values
* Gaps in data
* Wrong type of data
* Outliers
* Sharp curves
* Conflicting information within a dataset
* Conflicting information across datasets
* Wrongly derived data
* Internal inconsistency
* External inconsistency
* Wrong spatial data
* Unusable data, including non-standard abbreviations, ambiguous data, extraneous data, inconsistent data

Not all of these data smells are detectable in code. You may have to ask people about the data. You may have to compare it to another dataset yourself. Does the agency that uses the data produce reports from the data? Does your analysis match those reports? That will expose wrongly derived data, or wrong units, or mistakes you made with inclusion or exclusion.

But with several of these data smells, we can do them first, before we do anything else. 

We're going to examine three here as they apply to the PPP loan data we've been working with: wrong type, missing data and gaps in data.  

## Wrong Type

First, let's look at **Wrong Type Of Data**. 

We can sniff that out by looking at the output of `readr`. 

Let's load the tidyverse, then the Maryland slice of PPP loan data we've used previously. 

This time, we're going to load the data in a CSV format, which stands for comma separated values and is essentially a fancy structured text file. Each column in the csv is separated -- "delimited" -- by a comma from the next column. The file has a ".gz" extension on the end because it's zipped up to keep the file sizes smaller.

We're also going to introduce a new argument to our function that reads in the data, read_csv(), called "guess_max". As R reads in the csv file, it will attempt to make some calls on what "data type" to assign to each field: number, character, date, and so on. The "guess_max" argument says: look at the values in the whatever number of rows we specify before deciding which data type to assign. In this case, we'll pick 10. 

```{r}

# Remove scientific notation
options(scipen=999)
# Load the tidyverse
library(tidyverse)
# Load the data
ppp_maryland_loans <- read_csv("data/ppp_loan_data/processed/md/ppp_loans_md.csv.gz", guess_max=10)

```
Pay attention to the red warning that signals "one or more parsing issues." It advises us to run the problems() function to see what went wrong.  Let's do that. 

```{r}

problems(ppp_maryland_loans)

```

It produces a table of all the parsing problems. It has 87,927 rows, which means we have that many problems.  In almost every case here, the `readr` library has guessed that a given column was of a "logical" data type -- True or False. It did it based on very limited information -- only 1,000 rows.  So, when it hit a value that looked like a date, or a character string, it didn't know what to do.  So it just didn't read in that value correctly. 

Why? Look closely. The `readr` library makes guesses on what data type to assign to each column based on the first 1,000 rows of data.  

The easy way to fix this is to remove the guess_max argument, and let readr do its magic. 

```{r}
ppp_maryland_loans <- read_csv("data/ppp_loan_data/processed/md/ppp_loans_md.csv.gz")
```
This time, we got no parsing failures.  And if we examine the data types `readr` assigned to each column using glimpse(), they generally make sense. 

```{r}
glimpse(ppp_maryland_loans)
```

Things that should be characters -- like state, city, name -- are characters (chr). Things that should be numbers (dbl) -- like amount -- are numbers. Date columns -- like date_approved -- are stored as dates. 

There are some minor problem.  The id column is a good example.  It read in as a number (dbl), which makes sense, because it really is just a string of numbers.  But we'd never need to do math on these values; it wouldn't make sense to add two ids together, for example.  So it is probably best stored as a character. The opposite would be more problematic.  If something that should be stored as a number we want to do math on was stored as a character, we couldn't actually use it to do math. 

We can fix that pretty easily, by overwriting the column while changing the data type, using mutate()

```{r}

ppp_maryland_loans <- ppp_maryland_loans %>%
  mutate(id = as.character(id))

```

When we glimpse() the dataframe again, it's been changed

```{r}

glimpse(ppp_maryland_loans)

```

## Missing Data

The second smell we can find in code is **missing data**. 

We can do that by grouping and counting columns. In addition to identifying the presence of NA values, this method will also give us a sense of the distribution of values in those columns. 

Let's start with the "franchise" column. The following code groups by the franchise name column, counts the number in each group, and then sorts from highest to lowest.   

There are 192,959 NA values in this column.  This makes sense. Not every business will be a franchisee of a larger company like Subway or Dunkin'. In this case, the presence of so many NAs isn't really concerning. 

```{r}

ppp_maryland_loans %>% 
  group_by(franchise_name) %>% 
  summarise(
    count=n()
  ) %>%
  arrange(desc(count))
```
Now let's try the "forgiveness_amount" column, which represents the amount of the loan that was forgiven, or not required to be paid back. In this case, there are 135,073 NA values. The rest have different dollar amounts.  

```{r}

ppp_maryland_loans %>% 
  group_by(forgiveness_amount) %>% 
  summarise(
    count=n()
  ) %>%
  arrange(desc(count))
```
Do the 135,073 NAs represent loans that weren't forgiven? 

Do they represent loans that might have been forgiven, but the data is simply missing the amount of money?  

We could check the documentation, which isn't particularly helpful.  It only says the column represents "forgiveness amount." 

We could check the "forgiveness_date" column, to see how many NAs it has: 135,703, the same number as the number of NAs in "forgiveness_amount".   

```{r}

ppp_maryland_loans %>% 
  group_by(forgiveness_date) %>% 
  summarise(
    count=n()
  ) %>%
  filter(is.na(forgiveness_date)) %>%
  arrange(desc(count))
```
We can group by forgiveness_amount and forgiveness_date to determine whether one is NA when the other is NA.  Because this grouping has 135,073 rows, too, we know this to be the case. 

```{r}
ppp_maryland_loans %>%
  group_by(forgiveness_amount, forgiveness_date) %>%
  summarise(
    count=n()
  ) %>%
  filter(is.na(forgiveness_date)) %>%
  arrange(desc(count))

```

Before we decide to base a publishable finding on this column, we should call the custodian of the data to confirm our hypothesis of NA values in these columns, that they represent loans that have not been forgiven. 

## Gaps in data

Let's now look at **gaps in data**. It's been my experience that gaps in data often have to do with time, so let's first look at "date_approved", so we can see if there's any missing months, or huge differences in the number of loans by month. Let's start with Date. If we're going to work with dates, we should have `lubridate` handy for `floor_date`. The `floor_date` function will allow us to group by month, instead of a single day. 

```{r}
library(lubridate)
```


```{r}
ppp_maryland_loans %>% 
  mutate(month_year_approved = floor_date(date_approved, "month")) %>%
  group_by(month_year_approved) %>% 
   summarise(
    count=n()
  ) %>%
  arrange(month_year_approved)
```

So, our data starts in April 2020, the month that has more loans -- 45,040 -- than any other month.  That makes sense, as the program launched at the start of the pandemic, in April 2020.  The number of loans declines each month through August 2020, as the initial round of the program worked through the initial funding allocation.  

Then, there are no loans until January 2021. Does the four-month gap in loans in this dataset represent a problem? Are we missing a bunch of records?  

Probably not. The program was reauthorized a few times. For example, it expired in August 2020, before being reauthorized with new loans being given in January 2021.

It's good to be aware of all gaps in data, but they don't always represent a problem. 

## Supicious Outliers 

Any time you are going to focus on a column for analysis, you should check for suspicious values. Are there any unusually large values or unusually small values?  Are there any values that should not exist in the data?

Let's consider the loan amount column from our Maryland PPP data, and find the largest (max) and smallest (min) amount.  

```{r}

ppp_maryland_loans %>% 
  summarise(max_amount = max(amount),
            min_amount= min(amount))
```
The largest amount is \$10 million which is the maximum size of a loan under the program. If the max amount was larger than \$10 million, that would be suspicious.  Similarly, if the smallest amount was a negative number, that would also raise an eyebrow.  What about $6?  That does seem a little odd. Why would someone take out a loan for that amount of money?

Let's take a look at the full record set for any loan less than $100. 

```{r}

ppp_maryland_loans %>% 
 filter(amount < 100)
```
We have two businesses, "GETGFTD LLC" and "LEGACY SPINE AND PAIN LLC" that both got very small loans of \$6 and \$78. Scan through the columns. Let's check for internal consistency.  There are other columns in the data that have related information: initial_approval_amount, current_approval_amount and payroll_proceed. Let's select just those columns

```{r}

ppp_maryland_loans %>% 
 filter(amount < 100) %>%
  select(name, amount, initial_approval_amount, current_approval_amount, payroll_proceed)
```
Those are all the same value, so at least this number is internally consistent.  But is it correct? Did someone mistype it when entering the data?  

A call to the bank -- Bank of America in both cases -- or the companies could help resolve this mystery. Or a call to the SBA -- the owner of the data -- to ask about small values generally may help us understand if small values like this are suspicious or not. 

